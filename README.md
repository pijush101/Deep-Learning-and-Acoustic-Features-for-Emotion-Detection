# Deep-Learning-and-Acoustic-Features-for-Emotion-Detection

This project focuses on recognizing human emotions—like happiness, anger, or sadness—by analyzing voice recordings. It uses acoustic features such as MFCCs and Mel spectrograms, and feeds them into a deep learning model (CNN) trained on the RAVDESS dataset. The goal is to accurately detect emotions from speech, with real-world applications in virtual assistants, mental health tools, and smarter human-computer interaction.
